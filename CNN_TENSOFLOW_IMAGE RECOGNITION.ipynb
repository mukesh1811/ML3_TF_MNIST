{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIGIT RECOGNITION USING TENSORFLOW\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. We are using one such MNIST dataset to illustrate the convolutional neural network (CNN) using tensorflow in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-cddc202c415e>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From G:\\Python\\Anaconda_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From G:\\Python\\Anaconda_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:/Users/320/Python/CNN_Tensorflow/MNIST_Train\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Python\\Anaconda_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:/Users/320/Python/CNN_Tensorflow/MNIST_Train\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Python\\Anaconda_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting C:/Users/320/Python/CNN_Tensorflow/MNIST_Train\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/320/Python/CNN_Tensorflow/MNIST_Train\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Python\\Anaconda_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/320/Python/CNN_Tensorflow/MNIST_Train\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "CNN is the artifical neural network for image processing. There are 3 major layers in a CNN\n",
    "1. Convolutional Layers\n",
    "2. Pooling Layers\n",
    "3. Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the weights, biases and functions for different layers\n",
    "\n",
    "def weight(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tensors to be used in the code\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "# Creating a tensor for holding the predicted values of y\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Defining the Weights and Bias for first set Convolution and Pooling Layer.\n",
    "# 5, 5 and 32 are the default size used in a basic network\n",
    "W_conv1 = weight([5, 5, 1, 32])\n",
    "b_conv1 = bias([32])\n",
    "\n",
    "# Reshaping the input to pass through the conv layers. 28 - default no of pixel\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# The convolutional layer accepts the actual input, weights and bias. \n",
    "# it is then given RELU Activation \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool(h_conv1)\n",
    "\n",
    "\n",
    "# Defining the Weights and Bias for second set of Convolution and Pooling Layer\n",
    "# 5, 5 and 64 are the default size used in a basic network in the second layer, 32 is to accept the inpout from the previous layer\n",
    "W_conv2 = weight([5, 5, 32, 64])\n",
    "b_conv2 = bias([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool(h_conv2)\n",
    "\n",
    "# Defining the weight and Bias for the first Fully Connected Layer\n",
    "# 7*7*64 and 1024 are the default pixel measurement used\n",
    "W_fc1 = weight([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias([1024])\n",
    "\n",
    "# Reshaping the output from the fully connected layer to pull the output from the network\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Introducing the dropout inorder to avoid overfitting of out network\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Defining the weight and Bias for the final Fully Connected Layer\n",
    "# 1024 to accept the input from the previous layer and 10 is for the output classes\n",
    "W_fc2 = weight([1024, 10])\n",
    "b_fc2 = bias([10])\n",
    "\n",
    "# Passing the output of the fully connected layer to the softmax activation function\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualting the metrics for our network\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "\n",
    "# setting up the training step as 0.0001 for and using ADAM Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Tensors for the prediction and the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.100\n",
      "step 100, training accuracy 0.920\n",
      "step 200, training accuracy 0.920\n",
      "step 300, training accuracy 0.920\n",
      "step 400, training accuracy 0.900\n",
      "step 500, training accuracy 0.980\n",
      "step 600, training accuracy 0.960\n",
      "step 700, training accuracy 0.920\n",
      "step 800, training accuracy 0.980\n",
      "step 900, training accuracy 0.980\n",
      "test accuracy 0.967\n"
     ]
    }
   ],
   "source": [
    "#  Initializing the Variables and running the session\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Running the session for 1000 epochs but printing the output only for every 100 steps\n",
    "# we are outputing only the training accuracy after evey 100 epochs\n",
    "# Test accuracy is displayed at the end.\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess,feed_dict={x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %.3f\"%(i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "print(\"test accuracy %g\"%accuracy.eval(session=sess,feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
